{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.io.votable import parse\n",
    "from astroquery.casda import Casda\n",
    "from astroquery.utils.tap.core import TapPlus\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "def allDone():\n",
    "    display(Audio(url=\"http://www.mario-museum.net/sons/smb2_perdu.wav\", autoplay=True))\n",
    "\n",
    "\n",
    "def extractor(df, name):\n",
    "    \"\"\"\n",
    "    Lazy extractor for coordinates of named objects in dataframe.\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input dataframe containing the columns: `name`, `ra_deg_cont`, and `dec_deg_cont`.\n",
    "    - name (str): The name of the object to search for in the dataframe for coordinate extraction.\n",
    "    Returns:\n",
    "    - ra,dec (floats): Coordinates of the named object in the dataframe\n",
    "    \"\"\"\n",
    "    temp = df[df[\"name\"] == name]\n",
    "    ra = temp[\"ra_deg_cont\"].values[0]\n",
    "    dec = temp[\"dec_deg_cont\"].values[0]\n",
    "    return ra, dec\n",
    "\n",
    "\n",
    "def convert_xml_to_pandas(xml_file_name):\n",
    "    \"\"\"\n",
    "    Converts an XML file to a pandas DataFrame.\n",
    "    Parameters:\n",
    "    - xml_file_name (str): The name of the XML file to be converted.\n",
    "    Returns:\n",
    "    - pd.DataFrame: A pandas DataFrame containing the data from the XML file.\n",
    "    \"\"\"\n",
    "    votable = parse(xml_file_name)\n",
    "    table = votable.get_first_table()\n",
    "    bill = table.to_table(use_names_over_ids=True)\n",
    "    return bill.to_pandas()\n",
    "\n",
    "\n",
    "def extract_sb_number(filename):\n",
    "    \"\"\"\n",
    "    Searches a string of text for the scheduling block ID (SBID)number.\n",
    "    Parameters:\n",
    "    - filename (string): Text string of the inputted filename\n",
    "    Returns:\n",
    "    - SBID (string): Subset of the filename containing the SBID.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"SB(\\d+)\")\n",
    "    match = pattern.search(filename)\n",
    "\n",
    "    if match:\n",
    "        sb_number = match.group(1)\n",
    "        return sb_number\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def search_within_sky_separation(df, ra, dec, max_sep):\n",
    "    \"\"\"\n",
    "    Search a pandas DataFrame for rows with coordinates within a certain sky separation from a given location.\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame with \"ra\" and \"dec\" columns representing celestial coordinates.\n",
    "    - ra (float): Right ascension of the target location in degrees.\n",
    "    - dec (float): Declination of the target location in degrees.\n",
    "    - max_sep_arcsec (float): Maximum allowed angular separation in arcseconds.\n",
    "    Returns:\n",
    "    - pd.DataFrame: Subset of the input DataFrame containing rows within the specified sky separation.\n",
    "    \"\"\"\n",
    "    target_coord = SkyCoord(ra=ra * u.deg, dec=dec * u.deg, frame=\"icrs\")\n",
    "    df_coords = SkyCoord(\n",
    "        ra=np.array(df[\"ra_deg_cont\"]) * u.deg, dec=np.array(df[\"dec_deg_cont\"]) * u.deg, frame=\"icrs\"\n",
    "    )\n",
    "    separation_arcsec = target_coord.separation(df_coords).arcsec\n",
    "    result_df = df[separation_arcsec <= max_sep.value]\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def error_quad(flux, flux_err, rms, scale=0.06):\n",
    "    \"\"\"\n",
    "    Calculates the quadrature error.\n",
    "    Parameters:\n",
    "    - flux (float): Peak flux value from the fit\n",
    "    - flux_err (float): Uncertainty on the peak flux from the fit\n",
    "    - rms (float): Root mean square noise for the image\n",
    "    - scale (float): Fractional percentage by which to scale the flux - default 6%\n",
    "    Returns:\n",
    "    - error (float): the quadrature error from the fitted error, rms, and flux scale uncertainty\n",
    "    \"\"\"\n",
    "    return np.sqrt((flux_err**2) + (rms**2) + (scale * flux) ** 2)\n",
    "\n",
    "\n",
    "def scatter_plot(df, name):\n",
    "    \"\"\"\n",
    "    Scatter plot helper function to plot the light curve from a given dataframe containing observations\n",
    "    multiple frequencies and of varying lengths.\n",
    "    Parameters:\n",
    "    - pd.Dataframe: Dataframe containing the columns: flux_peak, flux_err_quad, obs_start, freq, obs_length\n",
    "    - name (string): Name of target object.\n",
    "    Returns:\n",
    "    - plot\n",
    "    \"\"\"\n",
    "    flux = df[\"flux_peak\"]\n",
    "    flux_errors = df[\"flux_err_quad\"]\n",
    "    dates = df[\"obs_start\"]\n",
    "    frequencies = df[\"freq\"]\n",
    "    observation_lengths = df[\"obs_length\"]\n",
    "\n",
    "    if len(flux) == 0:\n",
    "        print(\"There are no rows to plot, consider enlarging your search radius\")\n",
    "        return\n",
    "\n",
    "    norm = plt.Normalize(frequencies.min(), frequencies.max())\n",
    "    cmap = plt.get_cmap(\"viridis_r\")\n",
    "\n",
    "    sc = plt.scatter(dates, flux, c=frequencies, cmap=cmap, norm=norm, s=50)\n",
    "    plt.errorbar(dates, flux, yerr=flux_errors, fmt=\"none\", ecolor=\"gray\", capsize=2, alpha=0.4)\n",
    "\n",
    "    cbar = plt.colorbar(sc)\n",
    "    cbar.set_label(\"Frequency [MHz]\")\n",
    "\n",
    "    plt.xlabel(\"Dates [MJD]\")\n",
    "    plt.ylabel(\"Flux Density [mJy/beam]\")\n",
    "    plt.title(\"{}\".format(name))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"{}_Scatter_CASDA.pdf\".format(name), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_directory(directory_path):\n",
    "    if not os.path.exists(directory_path):\n",
    "        try:\n",
    "            os.makedirs(directory_path)\n",
    "            print(f\"Directory '{directory_path}' created successfully.\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory '{directory_path}': {e}\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory_path}' already exists.\")\n",
    "\n",
    "\n",
    "# Your OPAL username\n",
    "username = input(\"Enter your OPAL username\")  # You can hardcode your username/email here\n",
    "casda = Casda()\n",
    "casda.login(username=username)  # This will ask you to enter your OPAL password\n",
    "\n",
    "\n",
    "def _download_single_file(url, savedir, casda_obj, file_num, total_files):\n",
    "    \"\"\"\n",
    "    Helper function to download a single file (used for parallel downloads).\n",
    "\n",
    "    Parameters:\n",
    "    - url: URL to download\n",
    "    - savedir: Directory to save file\n",
    "    - casda_obj: Casda instance for downloading\n",
    "    - file_num: Current file number (for progress tracking)\n",
    "    - total_files: Total number of files\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of (success, filename, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download single file\n",
    "        filelist = casda_obj.download_files([url], savedir=savedir)\n",
    "        filename = filelist[0] if filelist else \"unknown\"\n",
    "        print(f\"  [{file_num}/{total_files}] Downloaded: {os.path.basename(filename)}\")\n",
    "        return (True, filename, None)\n",
    "    except Exception as e:\n",
    "        print(f\"  [{file_num}/{total_files}] ERROR downloading {url}: {e}\")\n",
    "        return (False, url, str(e))\n",
    "\n",
    "\n",
    "def _parallel_download_files(url_list, savedir, casda_obj, max_workers=4):\n",
    "    \"\"\"\n",
    "    Download multiple files in parallel using ThreadPoolExecutor.\n",
    "    Parameters:\n",
    "    - url_list: List of URLs to download\n",
    "    - savedir: Directory to save files\n",
    "    - casda_obj: Casda instance for downloading\n",
    "    - max_workers: Number of parallel download threads\n",
    "    Returns:\n",
    "    - List of successfully downloaded files\n",
    "    \"\"\"\n",
    "    downloaded_files = []\n",
    "    failed_downloads = []\n",
    "    total_files = len(url_list)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all download tasks\n",
    "        future_to_url = {\n",
    "            executor.submit(_download_single_file, url, savedir, casda_obj, i + 1, total_files): url\n",
    "            for i, url in enumerate(url_list)\n",
    "        }\n",
    "        # Process completed downloads as they finish\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                success, filename, error = future.result()\n",
    "                if success:\n",
    "                    downloaded_files.append(filename)\n",
    "                else:\n",
    "                    failed_downloads.append((url, error))\n",
    "            except Exception as e:\n",
    "                print(f\"  Unexpected error processing {url}: {e}\")\n",
    "                failed_downloads.append((url, str(e)))\n",
    "\n",
    "    print(f\"\\nDownload summary: {len(downloaded_files)}/{total_files} successful\")\n",
    "    if failed_downloads:\n",
    "        print(f\"Failed downloads ({len(failed_downloads)}):\")\n",
    "        for url, error in failed_downloads:\n",
    "            print(f\"  - {url}: {error}\")\n",
    "\n",
    "    return downloaded_files\n",
    "\n",
    "\n",
    "def casda_download(name, ra, dec, optimised=True, parallel_downloads=True, max_workers=4):\n",
    "    \"\"\"\n",
    "    Searches the CASDA archive for catalogues covering the given coordinates.\n",
    "    Downloads the catalogue .xml files for each scheduling block ID (SBID) into a new directory,\n",
    "    created with the given target name. Uses chunked batch staging and optional parallel downloads.\n",
    "\n",
    "    Parameters:\n",
    "    - name (string): Name of named object to search for database\n",
    "    - ra (float): Right ascension of the target location in degrees.\n",
    "    - dec (float): Declination of the target location in degrees.\n",
    "    - optimised (bool): Use optimised batch staging (default True). Set to False for original behavior.\n",
    "    - parallel_downloads (bool): Use parallel downloads (default True). Only works with optimised=True.\n",
    "    - max_workers (int): Number of parallel download threads (default 4). Increase for faster downloads.\n",
    "    Returns:\n",
    "    - df_list (list): Appended dataframes containing all catalogue rows from each SBID\n",
    "    \"\"\"\n",
    "\n",
    "    tap = TapPlus(url=\"https://casda.csiro.au/casda_vo_tools/tap\")\n",
    "\n",
    "    # This is the search function, here I'm searching only\n",
    "    # for continuum catalogues. You can change this to search\n",
    "    # for other data products instead\n",
    "    job = tap.launch_job_async(\n",
    "        (\"SELECT TOP 50000 * FROM ivoa.obscore where(dataproduct_subtype = 'catalogue.continuum.component')\")\n",
    "    )\n",
    "    r = job.get_results()\n",
    "    data = r[(r[\"quality_level\"] == \"GOOD\") | (r[\"quality_level\"] == \"UNCERTAIN\")]\n",
    "\n",
    "    # You have to do this step unless you have permission\n",
    "    # for embargoed data associated with you OPAL account login\n",
    "    public_data = Casda.filter_out_unreleased(data)\n",
    "\n",
    "    pubdat = public_data.to_pandas()\n",
    "    print(\"Number of rows: \", len(pubdat.index))\n",
    "\n",
    "    print(\"Source coordinates:\")\n",
    "    print(ra, dec)\n",
    "    example_source_coords = SkyCoord(ra * u.deg, dec * u.deg)\n",
    "    example_radius = 10 * u.deg\n",
    "    public_coords = SkyCoord(np.array(public_data[\"s_ra\"]) * u.deg, np.array(public_data[\"s_dec\"]) * u.deg)\n",
    "    seps = example_source_coords.separation(public_coords)\n",
    "    matches = np.where(seps < example_radius)[0]\n",
    "\n",
    "    matching_files = np.array(pubdat.iloc[matches][\"filename\"])\n",
    "    matching_files = [f for f in matching_files if \"checksum\" not in f]\n",
    "    print(f\"Found {len(matching_files)} matching files to download\")\n",
    "\n",
    "    casda_filepath = (\n",
    "        f\"/Downloads/{name}/\"  # creates a named folder for the target source in your downloads directory\n",
    "    )\n",
    "    create_directory(casda_filepath)\n",
    "\n",
    "    # Batch stage files in chunks to avoid URL length limits\n",
    "    if optimised and len(matching_files) > 0:\n",
    "        print(\"Staging files (chunked batch mode - optimised)...\")\n",
    "        url_list = []\n",
    "        chunk_size = 50  # Stage 50 files at a time (adjust if needed)\n",
    "        # Split matching files into chunks\n",
    "        matching_files_list = list(matching_files)\n",
    "        for i in range(0, len(matching_files_list), chunk_size):\n",
    "            chunk = matching_files_list[i : i + chunk_size]\n",
    "            print(\n",
    "                f\"Staging chunk {i//chunk_size + 1}/{(len(matching_files_list) + chunk_size - 1)//chunk_size} ({len(chunk)} files)...\"\n",
    "            )\n",
    "            chunk_data = public_data[np.isin(public_data[\"filename\"], chunk)]\n",
    "\n",
    "            try:\n",
    "                urls = casda.stage_data(chunk_data)\n",
    "                for url in urls:\n",
    "                    if url not in url_list:  # Add unique URLs only\n",
    "                        url_list.append(url)\n",
    "            except Exception as e:\n",
    "                print(f\"Error staging chunk: {e}\")\n",
    "                print(\"Falling back to individual staging for this chunk...\")\n",
    "                # Fallback to individual staging for this chunk\n",
    "                for mfile in chunk:\n",
    "                    try:\n",
    "                        pdata = public_data[public_data[\"filename\"] == mfile]\n",
    "                        url = casda.stage_data(pdata)\n",
    "                        if url not in url_list:\n",
    "                            url_list += url\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Error staging {mfile}: {e2}\")\n",
    "\n",
    "        print(f\"Staged {len(url_list)} total files\")\n",
    "    else:\n",
    "        # If your resources aren't suited to the optimisation you can stage files one by one (slower)\n",
    "        print(\"Staging files (individual mode - original)...\")\n",
    "        url_list = []\n",
    "        for mfile in matching_files:\n",
    "            if \"checksum\" not in mfile:\n",
    "                pdata = public_data[public_data[\"filename\"] == mfile]\n",
    "                url = casda.stage_data(pdata)\n",
    "                if url not in url_list:\n",
    "                    url_list += url\n",
    "\n",
    "    if optimised and parallel_downloads and len(url_list) > 1:\n",
    "        print(f\"Downloading {len(url_list)} files in parallel (max {max_workers} workers)...\")\n",
    "        _parallel_download_files(url_list, casda_filepath, casda, max_workers)\n",
    "    else:\n",
    "        print(\"Downloading files (sequential)...\")\n",
    "        filelist = casda.download_files(url_list, savedir=casda_filepath)\n",
    "    print(\"Downloads completed.\")\n",
    "    print(\"Generating dataframes.\")\n",
    "    obs_lookup = {}\n",
    "    for _, row in pubdat.iterrows():\n",
    "        obs_id = row[\"obs_id\"]\n",
    "        if obs_id not in obs_lookup:\n",
    "            obs_lookup[obs_id] = {\n",
    "                \"t_min\": row[\"t_min\"],\n",
    "                \"t_exptime\": row[\"t_exptime\"],\n",
    "                \"obs_collection\": row[\"obs_collection\"],\n",
    "            }\n",
    "    xml_file_path = casda_filepath\n",
    "    directory = os.fsencode(xml_file_path)\n",
    "    sbid_list = []\n",
    "    df_list = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".components.xml\"):\n",
    "            print(filename)\n",
    "            sbid = extract_sb_number(filename)\n",
    "            sbid_list.append(\"SB\" + sbid)\n",
    "            df = convert_xml_to_pandas(xml_file_path + filename)\n",
    "\n",
    "            obs_info_found = False\n",
    "            for prefix in [\"ASKAP-\", \"Beta-\"]:\n",
    "                obs_id = f\"{prefix}{sbid}\"\n",
    "                if obs_id in obs_lookup:\n",
    "                    obs_info = obs_lookup[obs_id]\n",
    "                    df[\"obs_start\"] = obs_info[\"t_min\"]\n",
    "                    df[\"obs_length\"] = obs_info[\"t_exptime\"]\n",
    "                    df[\"project\"] = obs_info[\"obs_collection\"]\n",
    "                    obs_info_found = True\n",
    "                    break\n",
    "\n",
    "            # If lookup dictionary method fails fallback to filtering pubdat each time\n",
    "            if not obs_info_found:\n",
    "                obs_info = pubdat[\n",
    "                    (pubdat[\"obs_id\"] == \"ASKAP-\" + sbid) | (pubdat[\"obs_id\"] == \"Beta-\" + sbid)\n",
    "                ]\n",
    "                if len(obs_info) > 0:\n",
    "                    df[\"obs_start\"] = obs_info[\"t_min\"].iloc[0]\n",
    "                    df[\"obs_length\"] = obs_info[\"t_exptime\"].iloc[0]\n",
    "                    df[\"project\"] = obs_info[\"obs_collection\"].iloc[0]\n",
    "                else:\n",
    "                   # Add default/null values if no match found\n",
    "                    print(f"Warning: No observation info found for SBID {sbid}")\n",
    "                    df['obs_start'] = np.nan\n",
    "                    df['obs_length'] = np.nan\n",
    "                    df['project'] = 'Unknown'\n",
    "\n",
    "            df_list.append(df)\n",
    "    return df_list\n",
    "\n",
    "\n",
    "def data_filter(name, ra, dec, df_list, sep=5, optimised=True):\n",
    "    \"\"\"\n",
    "    Filters the list of dataframes for sources within 5 arcseconds of the target coordinates.\n",
    "    Calculates the quadrature error which is more conservative than the fitted error and\n",
    "    accounts for flux scaling uncertainties. Builds a combined dataframe containing the\n",
    "    following columns: component_id, component_name, ra_deg_cont, dec_deg_cont,freq,\n",
    "    flux_peak, flux_peak_err, rms_image, obs_start,obs_length,flux_err_quad.\n",
    "    Pre-computes target coordinates and uses vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    - name (string): Name of target object.\n",
    "    - ra (float): Right ascension of the target location in degrees.\n",
    "    - dec (float): Declination of the target location in degrees.\n",
    "    - df_list (list): Appended dataframes containing all catalogue rows from each SBID\n",
    "    - sep (float): maximum separation between catalogued sources and target coordinates in arcsec - default 5\n",
    "    - optimised (bool): Use optimised vectorized operations (default True). Set to False for original behavior.\n",
    "    Returns:\n",
    "    - df_combined_sorted (pd.DataFrame): Combined and filtered dataframe sorted by observation data\n",
    "    - data_no_matches (list): Observations without matches within the maximum separation\n",
    "    \"\"\"\n",
    "    max_sep = sep * u.arcsec\n",
    "    if optimised:\n",
    "        target_coord = SkyCoord(ra=ra * u.deg, dec=dec * u.deg, frame=\"icrs\")\n",
    "    data_combined = []\n",
    "    data_no_matches = []\n",
    "    for index, df in enumerate(df_list):\n",
    "        if optimised:\n",
    "            if len(df) == 0:\n",
    "                data_no_matches.append(df_list[index])\n",
    "                continue\n",
    "\n",
    "            df_coords = SkyCoord(\n",
    "                ra=df[\"ra_deg_cont\"].values * u.deg, dec=df[\"dec_deg_cont\"].values * u.deg, frame=\"icrs\"\n",
    "            )\n",
    "            separations = target_coord.separation(df_coords)\n",
    "            mask = separations <= max_sep\n",
    "\n",
    "            if mask.any():\n",
    "                idx = np.where(mask)[0][0]\n",
    "                row = df.iloc[idx]\n",
    "                data_combined.append(\n",
    "                    [\n",
    "                        row[\"component_id\"],\n",
    "                        row[\"component_name\"],\n",
    "                        row[\"ra_deg_cont\"],\n",
    "                        row[\"dec_deg_cont\"],\n",
    "                        row[\"freq\"],\n",
    "                        row[\"flux_peak\"],\n",
    "                        row[\"flux_peak_err\"],\n",
    "                        row[\"rms_image\"],\n",
    "                        row[\"obs_start\"],\n",
    "                        row[\"obs_length\"],\n",
    "                        row[\"project\"],\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                data_no_matches.append(df_list[index])\n",
    "        else:\n",
    "            # non-vectorized method (slower)\n",
    "            matches = search_within_sky_separation(df, ra, dec, max_sep)\n",
    "            if len(matches) > 0:\n",
    "                data_combined.append(\n",
    "                    [\n",
    "                        matches.component_id.values[0],\n",
    "                        matches.component_name.values[0],\n",
    "                        matches.ra_deg_cont.values[0],\n",
    "                        matches.dec_deg_cont.values[0],\n",
    "                        matches.freq.values[0],\n",
    "                        matches.flux_peak.values[0],\n",
    "                        matches.flux_peak_err.values[0],\n",
    "                        matches.rms_image.values[0],\n",
    "                        matches.obs_start.values[0],\n",
    "                        matches.obs_length.values[0],\n",
    "                        matches.project.values[0],\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                data_no_matches.append(df_list[index])\n",
    "\n",
    "    df_combined = pd.DataFrame(\n",
    "        data_combined,\n",
    "        columns=[\n",
    "            \"component_id\",\n",
    "            \"component_name\",\n",
    "            \"ra_deg_cont\",\n",
    "            \"dec_deg_cont\",\n",
    "            \"freq\",\n",
    "            \"flux_peak\",\n",
    "            \"flux_peak_err\",\n",
    "            \"rms_image\",\n",
    "            \"obs_start\",\n",
    "            \"obs_length\",\n",
    "            \"project\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df_combined[\"flux_err_quad\"] = error_quad(\n",
    "        df_combined.flux_peak, df_combined.flux_peak_err, df_combined.rms_image\n",
    "    )\n",
    "\n",
    "    df_combined_sorted = df_combined.sort_values(by=[\"obs_start\"])\n",
    "    print(f\"Rows for {name} have been filtered and sorted.\")\n",
    "\n",
    "    return df_combined_sorted, data_no_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Source CASDA Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"M31\"\n",
    "ra, dec = 10.684708, 41.268750\n",
    "start = datetime.now()\n",
    "run_dt_stamp = start.strftime(\"%Y%m%d_%H%M\")\n",
    "df_list = casda_download(name, ra, dec, parallel_downloads=True, max_workers=8)\n",
    "df_combined_sorted, data_no_matches = data_filter(name, ra, dec, df_list, sep=5)\n",
    "scatter_plot(df_combined_sorted, name)\n",
    "df_combined_sorted.to_csv(f\"{name}_CASDA_matches_{run_dt_stamp}.csv\")\n",
    "end = datetime.now()\n",
    "elapsed = end - start\n",
    "print(f\"{name} CASDA executed in seconds: {elapsed.total_seconds()}\")\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Source CASDA Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assumes the inputted sources csv has the columns: name, ra_deg_cont, dec_deg_cont\n",
    "df = pd.read_csv(\"sources.csv\")\n",
    "names = df[\"name\"].to_list()\n",
    "\n",
    "for name in names:\n",
    "    start = datetime.now()\n",
    "    run_dt_stamp = start.strftime(\"%Y%m%d_%H%M\")\n",
    "    ra, dec = extractor(df, name)\n",
    "    df_list = casda_download(name, ra, dec, parallel_downloads=True, max_workers=4)\n",
    "    df_combined_sorted, data_no_matches = data_filter(name, ra, dec, df_list, sep=5)\n",
    "    scatter_plot(df_combined_sorted, name)\n",
    "    now = datetime.now()\n",
    "    run_dt_stamp = now.strftime(\"%Y%m%d_%H%M\")\n",
    "    df_combined_sorted.to_csv(f\"{name}_CASDA_matches_{run_dt_stamp}.csv\")\n",
    "    end = datetime.now()\n",
    "    elapsed = end - start\n",
    "    print(f\"{name} CASDA executed in seconds: {elapsed.total_seconds()}\")\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
