{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from datetime import datetime\n",
    "\n",
    "from astropy.io.votable import parse\n",
    "from astroquery.casda import Casda\n",
    "from astroquery.utils.tap.core import TapPlus\n",
    "from astroquery.utils.tap.core import Tap\n",
    "from astropy.io.votable import parse, parse_single_table\n",
    "\n",
    "import astropy.coordinates as coord\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "def allDone():\n",
    "    \"\"\"\n",
    "    Audio notifier for task completition with default sound file.\n",
    "    \"\"\"\n",
    "\n",
    "    display(Audio(url=\"http://www.mario-museum.net/sons/smb2_perdu.wav\", autoplay=True))\n",
    "\n",
    "\n",
    "def extractor(df, name):\n",
    "    \"\"\"\n",
    "    Lazy extractor for coordinates of named objects in dataframe.\n",
    "    Parameters:\n",
    "    - pd.Dataframe: Dataframe containing the columns: name, ra_deg_cont, dec_deg_cont\n",
    "    - name (string): Name of named object to search for in dataframe for coordinate extraction\n",
    "    Returns:\n",
    "    - ra,dec (floats): Coordinates of the named object in the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    temp = df[df[\"name\"] == name]\n",
    "    ra = temp[\"ra_deg_cont\"].values[0]\n",
    "    dec = temp[\"dec_deg_cont\"].values[0]\n",
    "    return ra, dec\n",
    "\n",
    "\n",
    "def convert_xml_to_pandas(xml_file_name):\n",
    "    \"\"\"\n",
    "    Coverts xml file to pandas dataframe.\n",
    "    Parameters:\n",
    "    - xml file\n",
    "    Returns:\n",
    "    - pd.DataFrame\n",
    "    \"\"\"\n",
    "    votable = parse(xml_file_name)\n",
    "    table = votable.get_first_table()\n",
    "    bill = table.to_table(use_names_over_ids=True)\n",
    "    return bill.to_pandas()\n",
    "\n",
    "\n",
    "def extract_sb_number(filename):\n",
    "    \"\"\"\n",
    "    Searches a string of text for the scheduling block ID (SBID)number.\n",
    "    Parameters:\n",
    "    - filename (string): Text string of the inputted filename\n",
    "    Returns:\n",
    "    - SBID (string): Subset of the filename containing the SBID.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"SB(\\d+)\")\n",
    "    match = pattern.search(filename)\n",
    "\n",
    "    if match:\n",
    "        sb_number = match.group(1)\n",
    "        return sb_number\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def search_within_sky_separation(df, ra, dec, max_sep):\n",
    "    \"\"\"\n",
    "    Search a pandas DataFrame for rows with coordinates within a certain sky separation from a given location.\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame with \"ra\" and \"dec\" columns representing celestial coordinates.\n",
    "    - ra (float): Right ascension of the target location in degrees.\n",
    "    - dec (float): Declination of the target location in degrees.\n",
    "    - max_sep_arcsec (float): Maximum allowed angular separation in arcseconds.\n",
    "    Returns:\n",
    "    - pd.DataFrame: Subset of the input DataFrame containing rows within the specified sky separation.\n",
    "    \"\"\"\n",
    "    # Create a SkyCoord object for the target location\n",
    "    target_coord = SkyCoord(ra=ra * u.deg, dec=dec * u.deg, frame=\"icrs\")\n",
    "    # Create SkyCoord objects for the coordinates in the DataFrame\n",
    "    df_coords = SkyCoord(\n",
    "        ra=np.array(df[\"ra_deg_cont\"]) * u.deg, dec=np.array(df[\"dec_deg_cont\"]) * u.deg, frame=\"icrs\"\n",
    "    )\n",
    "    # Calculate angular separation in arcseconds\n",
    "    separation_arcsec = target_coord.separation(df_coords).arcsec\n",
    "    # Filter DataFrame based on the separation\n",
    "    result_df = df[separation_arcsec <= max_sep.value]\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def error_quad(flux, flux_err, rms, scale=0.06):\n",
    "    \"\"\"\n",
    "    Calculates the quadrature error.\n",
    "    Parameters:\n",
    "    - flux (float): Peak flux value from the fit\n",
    "    - flux_err (float): Uncertainty on the peak flux from the fit\n",
    "    - rms (float): Root mean square noise for the image\n",
    "    - scale (float): Fractional percentage by which to scale the flux - default 6%\n",
    "    Returns:\n",
    "    - error (float): the quadrature error from the fitted error, rms, and flux scale uncertainty\n",
    "    \"\"\"\n",
    "    return np.sqrt((flux_err**2) + (rms**2) + (scale * flux) ** 2)\n",
    "\n",
    "\n",
    "def scatter_plot(df, name):\n",
    "    \"\"\"\n",
    "    Scatter plot helper function to plot the light curve from a given dataframe containing observations\n",
    "    multiple frequencies and of varying lengths.\n",
    "    Parameters:\n",
    "    - pd.Dataframe: Dataframe containing the columns: flux_peak, flux_err_quad, obs_start, freq, obs_length\n",
    "    - name (string): Name of target object.\n",
    "    Returns:\n",
    "    - plot\n",
    "    \"\"\"\n",
    "    flux = df[\"flux_peak\"]\n",
    "    flux_errors = df[\"flux_err_quad\"]\n",
    "    dates = df[\"obs_start\"]\n",
    "    frequencies = df[\"freq\"]\n",
    "    observation_lengths = df[\"obs_length\"]\n",
    "\n",
    "    if len(flux) == 0:\n",
    "        print(\"There are no rows to plot, consider enlarging your search radius\")\n",
    "        return\n",
    "\n",
    "    norm = plt.Normalize(frequencies.min(), frequencies.max())\n",
    "    cmap = plt.get_cmap(\"viridis_r\")\n",
    "\n",
    "    sc = plt.scatter(\n",
    "        dates, flux, c=frequencies, cmap=cmap, norm=norm, s=50\n",
    "    )  # *observation_lengths/observation_lengths.min(), edgecolor='black')\n",
    "    plt.errorbar(dates, flux, yerr=flux_errors, fmt=\"none\", ecolor=\"gray\", capsize=2, alpha=0.4)\n",
    "\n",
    "    cbar = plt.colorbar(sc)\n",
    "    cbar.set_label(\"Frequency [MHz]\")\n",
    "\n",
    "    plt.xlabel(\"Dates [MJD]\")\n",
    "    plt.ylabel(\"Flux Density [mJy/beam]\")\n",
    "    plt.title(\"{}\".format(name))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"{}_Scatter_CASDA.pdf\".format(name), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_directory(directory_path):\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        try:\n",
    "            # Create the directory\n",
    "            os.makedirs(directory_path)\n",
    "            print(f\"Directory '{directory_path}' created successfully.\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory '{directory_path}': {e}\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory_path}' already exists.\")\n",
    "\n",
    "\n",
    "def casda_download(name, ra, dec):\n",
    "    \"\"\"\n",
    "    Searches the CASDA archive for catalogues covering the given coordinates.\n",
    "    Downloads the catalogue .xml files for each scheduling block ID (SBID) into a new directory,\n",
    "    created with the given target name.\n",
    "    Parameters:\n",
    "    - name (string): Name of named object to search for database\n",
    "    - ra (float): Right ascension of the target location in degrees.\n",
    "    - dec (float): Declination of the target location in degrees.\n",
    "    Returns:\n",
    "    - df_list (list): Appended dataframes containing all catalogue rows from each SBID\n",
    "    \"\"\"\n",
    "    # Your OPAL username\n",
    "    username = input(\"Enter your OPAL username\")  # You can hardcode your username/email here\n",
    "    casda = Casda()\n",
    "    casda.login(username=username)  # This will ask you to enter your OPAL password\n",
    "\n",
    "    # Set up the TAP url\n",
    "    tap = TapPlus(url=\"https://casda.csiro.au/casda_vo_tools/tap\")\n",
    "\n",
    "    # This is the search function, here I'm searching only\n",
    "    # for continuum catalogues. You can change this to search\n",
    "    # for other data products instead\n",
    "    # job = tap.launch_job_async((\"SELECT TOP 50000 * FROM ivoa.obscore where(dataproduct_subtype = 'cont.restored.t0')\"))\n",
    "    job = tap.launch_job_async(\n",
    "        (\"SELECT TOP 50000 * FROM ivoa.obscore where(dataproduct_subtype = 'catalogue.continuum.component')\")\n",
    "    )\n",
    "    r = job.get_results()\n",
    "\n",
    "    # Here I keep only good or uncertain data\n",
    "    data = r[(r[\"quality_level\"] == \"GOOD\") | (r[\"quality_level\"] == \"UNCERTAIN\")]\n",
    "\n",
    "    # You have to do this step unless you have permission\n",
    "    # for embargoed data associated with you OPAL\n",
    "    # account login\n",
    "    public_data = Casda.filter_out_unreleased(data)\n",
    "\n",
    "    # Get the centre coords of all of the observations\n",
    "    # in the table\n",
    "    public_coords = SkyCoord(np.array(public_data[\"s_ra\"]) * u.deg, np.array(public_data[\"s_dec\"]) * u.deg)\n",
    "    # Get the file names\n",
    "    public_files = np.array(public_data[\"filename\"])\n",
    "\n",
    "    # This is the filepath where this script\n",
    "    # will save the products you download\n",
    "\n",
    "    casda_filepath = \"Downloads/{}/\".format(\n",
    "        name\n",
    "    )  # creates a named folder for the target source in your downloads directory\n",
    "    create_directory(casda_filepath)\n",
    "    # local\n",
    "    # casda_filepath = '/Downloads'\n",
    "    # I like pandas better\n",
    "    pubdat = public_data.to_pandas()\n",
    "    print(\"Number of rows: \", len(pubdat.index))\n",
    "\n",
    "    print(\"Source coordinates:\")  # print(\"Please enter the source coordinates\")\n",
    "    ra = ra  # float(input(\"RA [deg]: \"))\n",
    "    dec = dec  # float(input(\"Dec [deg]: \"))\n",
    "    print(ra, dec)\n",
    "\n",
    "    # The coordinates of the source you're interested in\n",
    "    example_source_coords = SkyCoord(ra * u.deg, dec * u.deg)\n",
    "    # This sets how far away from the centre of the image\n",
    "    # that you're searching\n",
    "    example_radius = 10 * u.deg  # float(input(\"Search radius [deg]: \"))* u.deg\n",
    "\n",
    "    # Find the rows in the public data table\n",
    "    # that are within example_radius of your\n",
    "    # source\n",
    "    seps = example_source_coords.separation(public_coords)\n",
    "    matches = np.where(seps < example_radius)[0]\n",
    "\n",
    "    matching_files = np.array(pubdat.iloc[matches][\"filename\"])\n",
    "    url_list = []\n",
    "\n",
    "    # This part stages the files you want to download\n",
    "    # so it sometimes takes a minute\n",
    "    for mfile in matching_files:\n",
    "        # I usually don't need the checksum files\n",
    "        if \"checksum\" not in mfile:\n",
    "            pdata = public_data[public_data[\"filename\"] == mfile]\n",
    "            url = casda.stage_data(pdata)\n",
    "            if url not in url_list:\n",
    "                url_list += url\n",
    "\n",
    "    filelist = casda.download_files(url_list, savedir=casda_filepath)\n",
    "\n",
    "    print(\"Downloads completed.\")\n",
    "\n",
    "    print(\"Generating dataframes.\")\n",
    "\n",
    "    xml_file_path = casda_filepath  #'CASDA_downloads/{}'.format(name)\n",
    "    directory = os.fsencode(xml_file_path)\n",
    "\n",
    "    sbid_list = []\n",
    "    df_list = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".components.xml\"):\n",
    "            print(filename)\n",
    "\n",
    "            sbid = extract_sb_number(filename)\n",
    "            sbid_list.append(\"SB\" + sbid)\n",
    "            df = convert_xml_to_pandas(xml_file_path + filename)\n",
    "            # obs_info = public_data[public_data['obs_id'] == 'ASKAP-'+sbid] #updated to include prefix\n",
    "            obs_info = pubdat[\n",
    "                (pubdat[\"obs_id\"] == \"ASKAP-\" + sbid) | (pubdat[\"obs_id\"] == \"Beta-\" + sbid)\n",
    "            ]  # updated to include prefix\n",
    "\n",
    "            df[\"obs_start\"] = obs_info[\"t_min\"].iloc[0]\n",
    "            df[\"obs_length\"] = obs_info[\"t_exptime\"].iloc[0]\n",
    "            df[\"project\"] = obs_info[\"obs_collection\"].iloc[0]\n",
    "            df_list.append(df)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    return df_list\n",
    "\n",
    "\n",
    "def data_filter(name, ra, dec, df_list, sep=5):\n",
    "    \"\"\"\n",
    "    Filters the list of dataframes for sources within 5 arcseconds of the target coordinates.\n",
    "    Calculates the quadrature error which is more conservative than the fitted error and\n",
    "    accounts for flux scaling uncertainties. Builds a combined dataframe containing the\n",
    "    following columns: component_id, component_name, ra_deg_cont, dec_deg_cont,freq,\n",
    "    flux_peak, flux_peak_err, rms_image, obs_start,obs_length,flux_err_quad.\n",
    "    Parameters:\n",
    "    - name (string): Name of target object.\n",
    "    - ra (float): Right ascension of the target location in degrees.\n",
    "    - dec (float): Declination of the target location in degrees.\n",
    "    - df_list (list): Appended dataframes containing all catalogue rows from each SBID\n",
    "    - sep (float): maximum separation between catalogued sources and target coordinates in arcsec - default 5\n",
    "    Returns:\n",
    "    - df_combined_sorted (pd.DataFrame): Combined and filtered dataframe sorted by observation data\n",
    "    - data_no_matches (list): Observations without matches within the maximum separation\n",
    "    \"\"\"\n",
    "    ra = ra\n",
    "    dec = dec\n",
    "    max_sep = sep * u.arcsec\n",
    "    combined_df = []\n",
    "\n",
    "    data_combined = []\n",
    "    data_no_matches = []\n",
    "    for index, df in enumerate(df_list):\n",
    "        matches = search_within_sky_separation(df, ra, dec, max_sep)\n",
    "        if len(matches) > 0:\n",
    "            data_combined.append(\n",
    "                [\n",
    "                    matches.component_id.values[0],\n",
    "                    matches.component_name.values[0],\n",
    "                    matches.ra_deg_cont.values[0],\n",
    "                    matches.dec_deg_cont.values[0],\n",
    "                    matches.freq.values[0],\n",
    "                    matches.flux_peak.values[0],\n",
    "                    matches.flux_peak_err.values[0],\n",
    "                    matches.rms_image.values[0],\n",
    "                    matches.obs_start.values[0],\n",
    "                    matches.obs_length.values[0],\n",
    "                    matches.project.values[0],\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            data_no_matches.append(df_list[index])\n",
    "\n",
    "    df_combined = pd.DataFrame(\n",
    "        data_combined,\n",
    "        columns=[\n",
    "            \"component_id\",\n",
    "            \"component_name\",\n",
    "            \"ra_deg_cont\",\n",
    "            \"dec_deg_cont\",\n",
    "            \"freq\",\n",
    "            \"flux_peak\",\n",
    "            \"flux_peak_err\",\n",
    "            \"rms_image\",\n",
    "            \"obs_start\",\n",
    "            \"obs_length\",\n",
    "            \"project\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df_combined[\"flux_err_quad\"] = error_quad(\n",
    "        df_combined.flux_peak, df_combined.flux_peak_err, df_combined.rms_image\n",
    "    )\n",
    "    df_combined_sorted = df_combined.sort_values(by=[\"obs_start\"])\n",
    "    print(\"Rows for {} have been filtered and sorted.\".format(name))\n",
    "    return df_combined_sorted, data_no_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Source CASDA Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"M31\"\n",
    "ra, dec = 10.684708, 41.268750\n",
    "df_list = casda_download(name, ra, dec)\n",
    "df_combined_sorted, data_no_matches = data_filter(name, ra, dec, df_list, sep=5)\n",
    "scatter_plot(df_combined_sorted, name)\n",
    "now = datetime.now()\n",
    "run_dt_stamp = now.strftime(\"%Y%m%d_%H%M\")\n",
    "df_combined_sorted.to_csv(\"{}_CASDA_matches_{}.csv\".format(name, run_dt_stamp))\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Source CASDA Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assumes the inputted sources csv has the columns: name, ra_deg_cont, dec_deg_cont\n",
    "\n",
    "df = pd.read_csv(\"sources.csv\")\n",
    "names = df[\"name\"].to_list()\n",
    "\n",
    "for name in names:\n",
    "    ra, dec = extractor(df, name)\n",
    "    df_list = casda_download(name, ra, dec)\n",
    "    f_combined_sorted, data_no_matches = data_filter(name, ra, dec, df_list, sep=5)\n",
    "    scatter_plot(df_combined_sorted, name)\n",
    "    now = datetime.now()\n",
    "    run_dt_stamp = now.strftime(\"%Y%m%d_%H%M\")\n",
    "    df_combined_sorted.to_csv(\"{}_CASDA_matches_{}.csv\".format(name, run_dt_stamp))\n",
    "\n",
    "allDone()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
