{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c292c7-859d-42ac-b10b-b7eddcbcee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from astropy.nddata import Cutout2D\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time\n",
    "from astropy.wcs.wcs import FITSFixedWarning \n",
    "import astropy.units as u\n",
    "from astroquery.casda import Casda\n",
    "from astroquery.utils.tap.core import TapPlus\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Suppress FITS fixed warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FITSFixedWarning)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def create_directory(directory_path):\n",
    "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(directory_path):\n",
    "        try:\n",
    "            os.makedirs(directory_path)\n",
    "            print(f\"Directory '{directory_path}' created successfully.\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory '{directory_path}': {e}\")\n",
    "\n",
    "\n",
    "def _download_single_file(url, savedir, casda_obj, file_num, total_files):\n",
    "    \"\"\"Helper function to download a single file (used for parallel downloads).\"\"\"\n",
    "    try:\n",
    "        filelist = casda_obj.download_files([url], savedir=savedir)\n",
    "        filename = filelist[0] if filelist else 'unknown'\n",
    "        print(f'  [{file_num}/{total_files}] Downloaded: {os.path.basename(filename)}')\n",
    "        return (True, filename, None)\n",
    "    except Exception as e:\n",
    "        print(f'  [{file_num}/{total_files}] ERROR downloading {url}: {e}')\n",
    "        return (False, url, str(e))\n",
    "\n",
    "\n",
    "def _parallel_download_files(url_list, savedir, casda_obj, max_workers=4):\n",
    "    \"\"\"Download multiple files in parallel using ThreadPoolExecutor.\"\"\"\n",
    "    downloaded_files = []\n",
    "    failed_downloads = []\n",
    "    total_files = len(url_list)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(_download_single_file, url, savedir, casda_obj, i+1, total_files): url \n",
    "            for i, url in enumerate(url_list)\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                success, filename, error = future.result()\n",
    "                if success:\n",
    "                    downloaded_files.append(filename)\n",
    "                else:\n",
    "                    failed_downloads.append((url, error))\n",
    "            except Exception as e:\n",
    "                print(f'  Unexpected error processing {url}: {e}')\n",
    "                failed_downloads.append((url, str(e)))\n",
    "    \n",
    "    print(f'\\nDownload summary: {len(downloaded_files)}/{total_files} successful')\n",
    "    if failed_downloads:\n",
    "        print(f'Failed downloads ({len(failed_downloads)}):')\n",
    "        for url, error in failed_downloads:\n",
    "            print(f'  - {url}: {error}')\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "\n",
    "def simplify_fits_header(hdu):\n",
    "    \"\"\"\n",
    "    Simplify a FITS header from 4D or 3D to 2D for cutout operations.\n",
    "    \n",
    "    Aggressively removes all WCS keywords related to axes > 2 \n",
    "    to prevent the 'operands could not be broadcast' error during WCS initialization.\n",
    "    \"\"\"\n",
    "    data_shape = hdu.data.shape\n",
    "    if len(data_shape) == 4:\n",
    "        # Standard ASKAP continuum: (Stokes, Freq, Dec, RA)\n",
    "        data = hdu.data[0, 0, :, :]\n",
    "    elif len(data_shape) == 3:\n",
    "        # 3D data: (Freq/Stokes, Dec, RA) - take the first plane\n",
    "        data = hdu.data[0, :, :]\n",
    "    elif len(data_shape) == 2:\n",
    "        # Already 2D\n",
    "        data = hdu.data\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected data shape: {data_shape}\")\n",
    "    \n",
    "    # Modify header: set NAXIS to 2 and remove all higher axis keywords\n",
    "    header = hdu.header.copy()\n",
    "    \n",
    "    # Base removal keys (CRPIX, CTYPE, etc.)\n",
    "    keys_to_remove = ['NAXIS3', 'NAXIS4', 'CRPIX3', 'CRPIX4', \n",
    "                      'CTYPE3', 'CTYPE4', 'CRVAL3', 'CRVAL4',\n",
    "                      'CDELT3', 'CDELT4', 'CUNIT3', 'CUNIT4', 'PC3_3', 'PC4_4']\n",
    "\n",
    "    #Remove all PC/CD matrix components referencing axes 3 and 4\n",
    "    for i in range(1, 5):\n",
    "        for j in range(1, 5):\n",
    "            if i > 2 or j > 2:\n",
    "                # Remove PC/CD terms that involve 3 or 4\n",
    "                keys_to_remove.append(f'PC{i}_{j}')\n",
    "                keys_to_remove.append(f'CD{i}_{j}')\n",
    "\n",
    "    # Apply removal\n",
    "    for key in keys_to_remove:\n",
    "        if key in header:\n",
    "            del header[key]\n",
    "\n",
    "    # Set primary axis count\n",
    "    header['NAXIS'] = 2\n",
    "    if 'WCSAXES' in header:\n",
    "        header['WCSAXES'] = 2\n",
    "    \n",
    "    return data, header\n",
    "\n",
    "\n",
    "def reconstruct_cutout_metadata(source_name, cutout_dir):\n",
    "    \"\"\"\n",
    "    Scans a directory for existing cutouts and rebuilds the metadata \n",
    "    dataframe required for plotting. Used for plot_only mode.\n",
    "    \"\"\"\n",
    "    clean_name = source_name.replace(' ', '').replace('(', '').replace(')', '')\n",
    "    search_pattern = f\"{cutout_dir}{clean_name}_*.fits\"\n",
    "    files = glob.glob(search_pattern)\n",
    "    \n",
    "    records = []\n",
    "    print(f\"Found {len(files)} existing cutouts for {source_name}\")\n",
    "    \n",
    "    for filepath in files:\n",
    "        try:\n",
    "            with fits.open(filepath) as hdul:\n",
    "                header = hdul[0].header\n",
    "                \n",
    "                # Extract info needed for sorting and plotting\n",
    "                obs_date = header.get('OBSDATE', header.get('DATE-OBS', ''))\n",
    "                mjd = header.get('MJD-OBS', 0)\n",
    "                \n",
    "                # If MJD is missing but date is present, calculate it\n",
    "                if mjd == 0 and obs_date:\n",
    "                    try:\n",
    "                        mjd = Time(obs_date).mjd\n",
    "                    except:\n",
    "                        mjd = 0\n",
    "                        \n",
    "                int_time = header.get('INT_TIME', 0) \n",
    "\n",
    "                records.append({\n",
    "                    'source_name': source_name,\n",
    "                    'cutout_filename': filepath,\n",
    "                    'image_filename': os.path.basename(filepath),\n",
    "                    'obs_date': obs_date,\n",
    "                    'obs_mjd': mjd,\n",
    "                    'integration_time': int_time\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping corrupt file {filepath}: {e}\")\n",
    "            \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def pre_filter_valid_cutouts(cutout_info):\n",
    "    \"\"\"\n",
    "    Checks each cutout file and removes records where data is all NaN or the file is corrupted.\n",
    "    This ensures only cutouts with actual data are passed to the plotter.\n",
    "    \"\"\"\n",
    "    valid_records = []\n",
    "    initial_count = len(cutout_info)\n",
    "    \n",
    "    for _, row in cutout_info.iterrows():\n",
    "        filepath = row['cutout_filename']\n",
    "        try:\n",
    "            with fits.open(filepath) as hdul:\n",
    "                data = hdul[0].data\n",
    "                # Check if there is AT LEAST ONE non-NaN pixel\n",
    "                if np.any(~np.isnan(data)):\n",
    "                    valid_records.append(row)\n",
    "                else:\n",
    "                    print(f\"  [Skipped] Cutout file is empty or all NaN: {os.path.basename(filepath)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [Skipped] Corrupt file during plot preparation: {os.path.basename(filepath)} ({e})\")\n",
    "            \n",
    "    final_count = len(valid_records)\n",
    "    if initial_count != final_count:\n",
    "        print(f\"Filtered {initial_count - final_count} invalid cutouts. {final_count} valid cutouts remain.\")\n",
    "\n",
    "    return pd.DataFrame(valid_records)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CASDA QUERY & DOWNLOAD FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def query_casda_images(ra, dec, search_radius=10*u.deg, \n",
    "                       image_type='cont.restored.t0',\n",
    "                       stokes='I'):\n",
    "    \"\"\"\n",
    "    Query CASDA for images covering the given coordinates, filtering by suffix.\n",
    "    \"\"\"\n",
    "    tap = TapPlus(url=\"https://casda.csiro.au/casda_vo_tools/tap\")\n",
    "    stokes_query = f\"pol_states = '/{stokes}/'\"\n",
    "    \n",
    "    print(f\"Querying CASDA for Stokes {stokes} images...\")\n",
    "    job = tap.launch_job_async(\n",
    "        f\"SELECT TOP 50000 * FROM ivoa.obscore WHERE \"\n",
    "        f\"({stokes_query} AND dataproduct_subtype = '{image_type}')\"\n",
    "    )\n",
    "    r = job.get_results()\n",
    "    \n",
    "    data = r[(r['quality_level'] == 'GOOD') | (r['quality_level'] == 'UNCERTAIN')]\n",
    "    public_data = Casda.filter_out_unreleased(data).to_pandas()\n",
    "    public_data = public_data[public_data['obs_id'].str.contains('ASKAP')]\n",
    "    \n",
    "    print(f'Total ASKAP images in archive: {len(public_data)}')\n",
    "    \n",
    "    # Spatial filter\n",
    "    source_coord = SkyCoord(ra * u.deg, dec * u.deg)\n",
    "    image_coords = SkyCoord(\n",
    "        np.array(public_data['s_ra']) * u.deg,\n",
    "        np.array(public_data['s_dec']) * u.deg\n",
    "    )\n",
    "    \n",
    "    seps = source_coord.separation(image_coords)\n",
    "    matches = np.where(seps < search_radius)[0]\n",
    "    matching_data = public_data.iloc[matches].copy()\n",
    "    print(f'Images covering source: {len(matching_data)}')\n",
    "    \n",
    "    # SUFFIX FILTER\n",
    "    suffix = \".restored.conv.fits\"\n",
    "    filtered_matches = matching_data[matching_data['filename'].str.endswith(suffix)].copy()\n",
    "    print(f'Filtered for \"{suffix}\": {len(filtered_matches)} images')\n",
    "    \n",
    "    return filtered_matches\n",
    "\n",
    "\n",
    "def download_casda_images(image_data, savedir, casda_obj, \n",
    "                          parallel=True, max_workers=4, chunk_size=50):\n",
    "    \"\"\"\n",
    "    Download CASDA images with optimized batch staging.\n",
    "    \"\"\"\n",
    "    create_directory(savedir)\n",
    "    if savedir[-1] != '/': savedir += '/'\n",
    "    \n",
    "    files_to_download = []\n",
    "    for filename in image_data['filename']:\n",
    "        if not os.path.isfile(f'{savedir}{filename}'):\n",
    "            files_to_download.append(filename)\n",
    "    \n",
    "    if len(files_to_download) == 0:\n",
    "        print('All files already downloaded.')\n",
    "        return []\n",
    "    \n",
    "    print(f'Files to download: {len(files_to_download)}')\n",
    "    \n",
    "    # Stage files in chunks\n",
    "    print('Staging files (chunked batch mode)...')\n",
    "    url_list = []\n",
    "    \n",
    "    for i in range(0, len(files_to_download), chunk_size):\n",
    "        chunk = files_to_download[i:i + chunk_size]\n",
    "        print(f'Staging chunk {i//chunk_size + 1}: {len(chunk)} files')\n",
    "        \n",
    "        chunk_data = image_data[image_data['filename'].isin(chunk)]\n",
    "        # Must pass a numpy record array or equivalent table, not a DataFrame\n",
    "        chunk_table = chunk_data.to_records(index=False) \n",
    "        \n",
    "        try:\n",
    "            urls = casda_obj.stage_data(chunk_table)\n",
    "            for url in urls:\n",
    "                if 'checksum' not in url and url not in url_list:\n",
    "                    url_list.append(url)\n",
    "        except Exception as e:\n",
    "            print(f'Error staging chunk: {e}')\n",
    "            # Fallback to individual staging if batch fails\n",
    "            for filename in chunk:\n",
    "                try:\n",
    "                    file_data = image_data[image_data['filename'] == filename]\n",
    "                    file_table = file_data.to_records(index=False)\n",
    "                    urls = casda_obj.stage_data(file_table) # Note: returns a list\n",
    "                    for url in urls:\n",
    "                        if url not in url_list:\n",
    "                             url_list.append(url)\n",
    "                except Exception as e2:\n",
    "                    print(f'Error staging {filename}: {e2}')\n",
    "    \n",
    "    print(f'Staged {len(url_list)} files')\n",
    "    \n",
    "    # Download files\n",
    "    if parallel and len(url_list) > 1:\n",
    "        print(f'Downloading {len(url_list)} files in parallel (max {max_workers} workers)...')\n",
    "        downloaded = _parallel_download_files(url_list, savedir, casda_obj, max_workers)\n",
    "    else:\n",
    "        print('Downloading files (sequential)...')\n",
    "        # astroquery.casda.download_files expects a list of staged URLs\n",
    "        downloaded = casda_obj.download_files(url_list, savedir=savedir)\n",
    "    \n",
    "    return downloaded\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CUTOUT GENERATION (WITH WCS & BEAM)\n",
    "# ==============================================================================\n",
    "\n",
    "def make_cutouts(sources_df, image_data, \n",
    "                 image_dir, cutout_dir,\n",
    "                 cutout_size=4.0*u.arcmin,\n",
    "                 ra_col='ra', dec_col='dec',\n",
    "                 pmra_col='pmra', pmdec_col='pmdec',\n",
    "                 epoch_col='epoch', name_col='name',\n",
    "                 apply_proper_motion=True):\n",
    "    \"\"\"\n",
    "    Create cutouts for sources from CASDA images, incorporating all necessary fixes.\n",
    "    Cutouts consisting of only NaN values are NOT saved or recorded.\n",
    "    \"\"\"\n",
    "    create_directory(cutout_dir)\n",
    "    \n",
    "    if image_dir[-1] != '/': image_dir += '/'\n",
    "    if cutout_dir[-1] != '/': cutout_dir += '/'\n",
    "    \n",
    "    cutout_records = []\n",
    "    \n",
    "    for idx, img_row in image_data.iterrows():\n",
    "        filename = img_row['filename']\n",
    "        filepath = f\"{image_dir}{filename}\"\n",
    "        \n",
    "        if not os.path.isfile(filepath):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        t_min_val = img_row['t_min']\n",
    "        if not np.isfinite(t_min_val):\n",
    "            print(f\"  WARNING: Skipping image {filename} due to invalid t_min (MJD).\")\n",
    "            continue\n",
    "            \n",
    "        img_epoch = Time(t_min_val, format='mjd')\n",
    "        \n",
    "        # Find sources in field\n",
    "        img_coord = SkyCoord(img_row['s_ra'] * u.deg, img_row['s_dec'] * u.deg)\n",
    "        sources_in_field = [src for _, src in sources_df.iterrows() \n",
    "                            if SkyCoord(src[ra_col] * u.deg, src[dec_col] * u.deg).separation(img_coord) < 6 * u.deg]\n",
    "        \n",
    "        if len(sources_in_field) == 0:\n",
    "            continue\n",
    "        \n",
    "        hdul = None\n",
    "        try:\n",
    "            hdul = fits.open(filepath)\n",
    "            hdu = hdul[0]\n",
    "            original_header = hdu.header # Save original header reference\n",
    "            \n",
    "            data, header = simplify_fits_header(hdu)\n",
    "            wcs = WCS(header) \n",
    "            \n",
    "            pixel_scale = np.abs(header['CDELT1']) * u.deg\n",
    "            # Calculate pixel size dynamically\n",
    "            cutout_pixels = int((cutout_size / pixel_scale).to(u.dimensionless_unscaled).value)\n",
    "            \n",
    "            for src in sources_in_field:\n",
    "                src_name = src[name_col]\n",
    "                clean_name = src_name.replace(' ', '').replace('(', '').replace(')', '')\n",
    "                cutout_filename = f\"{cutout_dir}{clean_name}_{filename}\"\n",
    "                \n",
    "                if os.path.isfile(cutout_filename):\n",
    "                    print(f\"    Skipping existing: {clean_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Apply Proper Motion\n",
    "                ra_val = src[ra_col] # Use raw series access\n",
    "                dec_val = src[dec_col]\n",
    "                src_coord = SkyCoord(ra_val * u.deg, dec_val * u.deg) # Default (J2000)\n",
    "                \n",
    "                if apply_proper_motion and pmra_col in src and pmdec_col in src:\n",
    "                    try:\n",
    "                        # Ensure PM inputs are single scalar floats\n",
    "                        pmra_val = float(np.nan_to_num(src[pmra_col], nan=0.0))\n",
    "                        pmdec_val = float(np.nan_to_num(src[pmdec_col], nan=0.0))\n",
    "\n",
    "                        src_coord = SkyCoord(\n",
    "                            ra_val * u.deg, dec_val * u.deg,\n",
    "                            pm_ra_cosdec=pmra_val * u.mas / u.yr,\n",
    "                            pm_dec=pmdec_val * u.mas / u.yr,\n",
    "                            frame='icrs', obstime=Time(src[epoch_col]), distance=1.0 * u.pc\n",
    "                        ).apply_space_motion(img_epoch)\n",
    "                    except Exception as pm_e:\n",
    "                        print(f\"    WARNING: Proper motion calculation failed ({pm_e}). Using J2000 coordinates.\")\n",
    "                        # src_coord remains the default J2000 assignment from above\n",
    "                        pass\n",
    "                    \n",
    "                try:\n",
    "                    # Cutout is performed on the 2D data using the 2D WCS object\n",
    "                    cutout = Cutout2D(data, src_coord, size=cutout_pixels, wcs=wcs)\n",
    "                    \n",
    "                    if not np.any(~np.isnan(cutout.data)):\n",
    "                        print(f\"    Skipping empty cutout for {src_name} (All NaNs)\")\n",
    "                        continue\n",
    "\n",
    "                    # Prepare new header\n",
    "                    cutout_hdu = fits.PrimaryHDU(data=cutout.data)\n",
    "                    cutout_hdu.header.update(cutout.wcs.to_header())\n",
    "                    \n",
    "                    keys_to_copy = ['BMAJ', 'BMIN', 'BPA', 'BUNIT', 'TELESCOP', 'INSTRUME']\n",
    "                    for key in keys_to_copy:\n",
    "                        if key in original_header:\n",
    "                            cutout_hdu.header[key] = original_header[key]\n",
    "                    \n",
    "                    # Add Custom Keys\n",
    "                    cutout_hdu.header['OBJECT'] = src_name\n",
    "                    cutout_hdu.header['OBSDATE'] = img_epoch.isot\n",
    "                    cutout_hdu.header['MJD-OBS'] = img_row['t_min'] # Ensure MJD is in cutout\n",
    "                    cutout_hdu.header['INT_TIME'] = img_row['t_exptime']\n",
    "                    \n",
    "                    cutout_hdu.writeto(cutout_filename, overwrite=True)\n",
    "                    \n",
    "                    cutout_records.append({\n",
    "                        'source_name': src_name,\n",
    "                        'image_filename': filename,\n",
    "                        'cutout_filename': cutout_filename,\n",
    "                        'obs_date': img_epoch.isot,\n",
    "                        'obs_mjd': img_row['t_min'],\n",
    "                        'integration_time': img_row['t_exptime']\n",
    "                    })\n",
    "                    print(f\"    Created cutout: {clean_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Failed cutout for {src_name}: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing image: {e}\")\n",
    "        finally:\n",
    "            if hdul: hdul.close() # Ensure file is closed\n",
    "    \n",
    "    return pd.DataFrame(cutout_records)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PLOTTING & WORKFLOW\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_cutout_grid(source_name, cutout_info, cutout_dir,\n",
    "                     save_path=None, close_image=False)\n",
    "                     target_ra=None, target_dec=None):\n",
    "    \"\"\"\n",
    "    Plot a grid of cutouts. Assumes cutout_info contains only valid records\n",
    "    (empty/corrupt files should be filtered out before calling this function).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_ra, target_dec : float, optional\n",
    "    If provided, adds a crosshair marker at this position\n",
    "    \"\"\"\n",
    "    if cutout_dir[-1] != '/': cutout_dir += '/'\n",
    "    if save_path is None: save_path = cutout_dir\n",
    "    elif save_path[-1] != '/': save_path += '/'\n",
    "    \n",
    "    # Sort and prepare the final set of valid cutouts\n",
    "    src_cutouts = cutout_info[cutout_info['source_name'] == source_name].sort_values(by='obs_mjd')\n",
    "    n_cutouts = len(src_cutouts)\n",
    "    \n",
    "    if n_cutouts == 0:\n",
    "        print(f\"No valid cutouts found for {source_name} after filtering.\")\n",
    "        return\n",
    "    \n",
    "    # Grid setup\n",
    "    n_cols = 3\n",
    "    n_rows = (n_cutouts + n_cols - 1) // n_cols\n",
    "    figsize = (12, 3.8 * n_rows) # Dynamic height\n",
    "        \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    \n",
    "    # Ensure axes is iterable and correctly shaped\n",
    "    if n_cutouts == 1: axes = np.array([axes])\n",
    "    if n_rows == 1 and n_cols > 1: axes = axes.reshape(1, -1)\n",
    "    elif n_cols == 1 and n_rows > 1: axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Plot each cutout\n",
    "    for idx, (_, row) in enumerate(src_cutouts.iterrows()):\n",
    "        ax = axes.flat[idx]\n",
    "        \n",
    "        try:\n",
    "            with fits.open(row['cutout_filename']) as hdul:\n",
    "                header = hdul[0].header\n",
    "                data = hdul[0].data\n",
    "                \n",
    "                # Get Beam Parameters\n",
    "                bmaj = header.get('BMAJ', 0.0) # Major axis (deg)\n",
    "                bmin = header.get('BMIN', 0.0) # Minor axis (deg)\n",
    "                bpa = header.get('BPA', 0.0)   # Position angle (deg)\n",
    "                \n",
    "                # Get Pixel Scale\n",
    "                pix_scale = np.abs(header.get('CDELT2', header.get('CD2_2', 0.0)))\n",
    "            \n",
    "            # Since we pre-filtered, we can proceed directly to plotting\n",
    "            valid_data = data[~np.isnan(data)]\n",
    "            \n",
    "            # Set Vmin/Vmax using 1st and 99th percentile for robust scaling\n",
    "            vmin, vmax = np.nanpercentile(valid_data, [1, 99])\n",
    "            \n",
    "            im = ax.imshow(data, cmap='magma', origin='lower', vmin=vmin, vmax=vmax)\n",
    "            cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            cbar.set_label(header.get('BUNIT', 'Jy/beam'), fontsize=8)\n",
    "            cbar.ax.tick_params(labelsize=7)\n",
    "            \n",
    "            # Configure RA/Dec axes\n",
    "            ax.coords[0].set_axislabel('RA (J2000)', fontsize=8)\n",
    "            ax.coords[1].set_axislabel('Dec (J2000)', fontsize=8)\n",
    "            ax.coords[0].set_ticklabel(size=7)\n",
    "            ax.coords[1].set_ticklabel(size=7)\n",
    "            ax.coords[0].set_major_formatter('hh:mm:ss.s')\n",
    "            ax.coords[1].set_major_formatter('dd:mm:ss')\n",
    "            \n",
    "            # Add crosshair marker at target position if provided\n",
    "           if target_ra is not None and target_dec is not None:\n",
    "                target_coord = SkyCoord(target_ra * u.deg, target_dec * u.deg)\n",
    "                ax.plot_coord(target_coord, 'w+', markersize=12, markeredgewidth=2, markeredgecolor='gray', \n",
    "                            label='Target', zorder=15)\n",
    "            \n",
    "            if bmaj > 0 and pix_scale > 0:\n",
    "                major_pix = bmaj / pix_scale\n",
    "                minor_pix = bmin / pix_scale\n",
    "                \n",
    "                # Position: 10% from bottom-left corner\n",
    "                beam_x = data.shape[1] * 0.1\n",
    "                beam_y = data.shape[0] * 0.1\n",
    "                \n",
    "                # Ellipse (angle is 90 + BPA for typical radio display where North is up)\n",
    "                beam_patch = patches.Ellipse(\n",
    "                    (beam_x, beam_y), \n",
    "                    width=minor_pix, \n",
    "                    height=major_pix, \n",
    "                    angle=90 + bpa, \n",
    "                    edgecolor='cyan',    # High contrast color\n",
    "                    facecolor='none',\n",
    "                    hatch='///',         \n",
    "                    linewidth=1.5,\n",
    "                    zorder=10             \n",
    "                )\n",
    "                ax.add_patch(beam_patch)\n",
    "                \n",
    "                # Set axis limits to match the data extent (removes blank space)\n",
    "                ax.set_xlim(-0.5, data.shape[1] - 0.5)\n",
    "                ax.set_ylim(-0.5, data.shape[0] - 0.5)\n",
    "                \n",
    "                # Add marker at center\n",
    "                center_x = data.shape[1] // 2\n",
    "                center_y = data.shape[0] // 2\n",
    "                rect = patches.Rectangle((center_x-major_pix/2, center_y-major_pix/2), 1*major_pix, 1*major_pix,\n",
    "                                         linewidth=1, edgecolor='white', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "            # Title (Date | SBID)\n",
    "            obs_date = str(row['obs_date']).split('T')[0]\n",
    "            int_time = round(row['integration_time'] / 60, 1)                \n",
    "            sbid_match = re.search(r'(SB\\d+)', row['image_filename'])\n",
    "            sbid_str = sbid_match.group(1) if sbid_match else ''\n",
    "            ax.set_title(f\"{obs_date} | {sbid_str}\\n{int_time} min\", fontsize=10)\n",
    "            \n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "        except Exception as e:\n",
    "            # This catch should only happen if a file is genuinely corrupt\n",
    "            ax.text(0.5, 0.5, 'Error', ha='center', va='center', transform=ax.transAxes)\n",
    "            print(f\"Plot Error for {row['image_filename']}: {e}\")\n",
    "            \n",
    "    # Clear empty plots at the end of the grid\n",
    "    for idx in range(n_cutouts, n_rows * n_cols):\n",
    "        fig.delaxes(axes.flat[idx])\n",
    "    \n",
    "    fig.suptitle(source_name, fontsize=14, y=0.99)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    clean_name = source_name.replace(' ', '').replace('(', '').replace(')', '')\n",
    "    save_file = f\"{save_path}{clean_name}_cutouts.png\"\n",
    "    fig.savefig(save_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved figure: {save_file}\")\n",
    "    \n",
    "    if close_image:\n",
    "        plt.close(fig)\n",
    "\n",
    "def process_single_source(name, ra, dec, \n",
    "                          image_dir, cutout_dir,\n",
    "                          casda_obj=None,\n",
    "                          pmra=0.0, pmdec=0.0, epoch='J2000',\n",
    "                          stokes='I',\n",
    "                          search_radius=10*u.deg,\n",
    "                          cutout_size=4.0*u.arcmin,\n",
    "                          parallel=True, max_workers=4,\n",
    "                          plot_only=False):\n",
    "    \"\"\"\n",
    "    Complete workflow for a single source.\n",
    "    Set plot_only=True to skip CASDA query/download and just plot existing files.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {name}\")\n",
    "    if plot_only:\n",
    "        print(\"(PLOT ONLY MODE - Skipping CASDA Query & Download)\")\n",
    "    else:\n",
    "        print(f\"Coordinates: RA={ra:.4f}°, Dec={dec:.4f}°\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    if plot_only:\n",
    "        print(\"Step 1/3: Reconstructing metadata from existing files...\")\n",
    "        cutout_info = reconstruct_cutout_metadata(name, cutout_dir)\n",
    "            \n",
    "    else:\n",
    "        sources_df = pd.DataFrame({\n",
    "            'name': [name], 'ra': [ra], 'dec': [dec], \n",
    "            'pmra': [pmra], 'pmdec': [pmdec], 'epoch': [epoch]\n",
    "        })\n",
    "        \n",
    "        # Query CASDA\n",
    "        print(\"Step 1/4: Querying CASDA for images...\")\n",
    "        if casda_obj is None:\n",
    "            raise ValueError(\"casda_obj is required when plot_only=False\")\n",
    "\n",
    "        image_data = query_casda_images(ra, dec, \n",
    "                                        search_radius=search_radius,\n",
    "                                        stokes=stokes)\n",
    "        \n",
    "        if len(image_data) == 0:\n",
    "            print(f\"No images found for {name}\")\n",
    "            return None\n",
    "        \n",
    "        # Download images\n",
    "        print(\"\\nStep 2/4: Downloading images...\")\n",
    "        download_casda_images(image_data, image_dir, casda_obj,\n",
    "                            parallel=parallel, max_workers=max_workers)\n",
    "        \n",
    "        # Create cutouts\n",
    "        print(\"\\nStep 3/4: Creating cutouts...\")\n",
    "        cutout_info = make_cutouts(sources_df, image_data, \n",
    "                                image_dir, cutout_dir,\n",
    "                                cutout_size=cutout_size,\n",
    "                                apply_proper_motion=(pmra != 0.0 or pmdec != 0.0))\n",
    "    \n",
    "    if cutout_info is None or len(cutout_info) == 0:\n",
    "        print(f\"No cutouts available for {name}\")\n",
    "        return None\n",
    "\n",
    "    # Filter out any files that were created in a previous run but contain no data\n",
    "    print(\"\\nStep 3/4 (or 2/3): Filtering out empty cutouts...\")\n",
    "    cutout_info = pre_filter_valid_cutouts(cutout_info)\n",
    "    \n",
    "    if len(cutout_info) == 0:\n",
    "        print(f\"No valid cutouts remaining for {name} after filtering.\")\n",
    "        return None\n",
    "    \n",
    "    # Plot cutouts\n",
    "    print(\"\\nStep 4/4 (or 3/3): Creating visualization...\")\n",
    "    plot_cutout_grid(name, cutout_info, cutout_dir)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ Complete! Displaying {len(cutout_info)} valid cutouts for {name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return cutout_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1db1a5-a510-462c-a138-7a6b7b8b8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from casda_cutout_workflow import process_single_source, Casda, u\n",
    "\n",
    "casda = Casda()\n",
    "casda.login(username='<INSERT YOUR OPAL EMAIL ADDRESS HERE>')\n",
    "\n",
    "# --- Full Run (Query, Download, Cutout, Plot) ---\n",
    "# This will overwrite existing cutouts and ensure headers are correct.\n",
    "\n",
    "start = datetime.now()\n",
    "run_dt_stamp = start.strftime(\"%Y%m%d_%H%M\")\n",
    "name = \"SN 2012dy\"\n",
    "ra, dec = 319.71125, -57.64514\n",
    "cutout_info = process_single_source(\n",
    "    name=name,\n",
    "    ra=ra,\n",
    "    dec=dec,\n",
    "    image_dir=f'./CASDA_Cutouts/{name}/images/',\n",
    "    cutout_dir=f'./CASDA_Cutouts/{name}/cutouts/',\n",
    "    casda_obj=casda,\n",
    "    plot_only=False,  # <--- MUST BE FALSE TO GENERATE NEW CUTOUTS\n",
    "    cutout_size=1.0*u.arcmin,\n",
    "    search_radius=3.0*u.deg,\n",
    "    max_workers=8\n",
    ")\n",
    "end = datetime.now()\n",
    "elapsed = end - start\n",
    "print(f\"{name} CASDA executed in seconds: {elapsed.total_seconds()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51dd5b-eef2-4a2d-be4c-e21c0cb5d23c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Plot Only Run (If files already exist) ---\n",
    "# If you just want to regenerate the figure quickly:\n",
    "name = \"SN 2012dy\"\n",
    "ra, dec = 319.71125, -57.64514\n",
    "cutout_info = process_single_source(\n",
    "    name=name,\n",
    "    ra=ra,\n",
    "    dec=dec,\n",
    "    image_dir=f'./CASDA_Cutouts/{name}/images/',\n",
    "    cutout_dir=f'./CASDA_Cutouts/{name}/cutouts/',\n",
    "    plot_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da37b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
